# Copyright 2024 Google LLC.
# (Tu licencia aquí)

# ===============================================================
# SECCIÓN 1: CONFIGURACIÓN DEL EXPERIMENTO Y DATOS
# ===============================================================

# --- Configuración General del Experimento ---
seed: 42
do_train: true
do_test: false
task: finetune  # Correcto: usamos el modo de entrenamiento supervisado con etiquetas.
experiment_name: mi_experimento_final_v1
output_dir: ./outputs/

finetune_initialize_from: random

# --- Configuración del Dataset ---
dataset_name: "custom"
data_dir: "/content/lanistr/"
category: "multi_dataset"

# --- Definición del Esquema de Datos ---
data_schema:
  # Unimos las 4 fases de la conversación para un contexto completo
  text_cols:
    - 'apertura'
    - 'interaccion'
    - 'resultado'
    - 'cierre'
  # Usamos todas las características numéricas
  numeric_cols:
    - 'num_word_text'
    - 'count_dialogue_cliente'
    - 'num_word_text_captador'
    - 'num_word_text_cliente'
    - 'word_per_dialogue_operador'
    - 'proportion_dialogue_captador'
  # Tratamos las columnas de sentimiento como categóricas (ej. 0 y 1)
  categorical_cols:
    - 'sentiment_text_POS'
    - 'sentiment_cliente_POS'
    - 'sentiment_captador_POS'
  # La columna 'result' es claramente el objetivo a predecir
  target_col: 'result'

# --- Configuración de Modalidades ---
image: false
time: false
text: true
tab: true

# ===============================================================
# SECCIÓN 2: HIPERPARÁMETROS DEL MODELO Y ENTRENAMIENTO
# ===============================================================

# --- Parámetros de la Tarea ---
train_batch_size: 32
eval_batch_size: 128
test_batch_size: 256
num_classes: 2
perf_metric: auroc

# --- Optimizador y Scheduler ---
scheduler:
  num_epochs: 50
  warmup_epochs: 5
optimizer:
  # ¡IMPORTANTE! Tasa de aprendizaje baja para entrenar BERT de forma estable.
  learning_rate: 2e-5
  weight_decay: 0.1
  clip_value: 5.0

# --- Arquitectura del Modelo ---

# Dimensiones Unificadas
projection_dim: 768

# classifier head
classifier_hidden_dim: 768

# simsiam pretraining projector and predictor
projection_type: SimSiam
predictor_hidden_dim: 512
predictor_out_dim: 2048

# Text Encoder
text_encoder_name: bert-base-uncased
text_encoder_pretrained: true
text_embedding_dim: 768
max_token_length: 512
mlm_probability: 0.15
# ¡IMPORTANTE! Activado para que aprenda de tus datos.
text_encoder_trainable: true
text_proj_trainable: true

# Tabular Encoder
tabular_encoder_name: tabnet
tabular_output_dim: 768
tabular_embedding_dim: 64
tabular_cat_emb_dim: 3
tabular_mask_type: sparsemax
tabular_n_d: 64
tabular_n_a: 64
# ¡IMPORTANTE! Activado para que aprenda de tus datos.
tabular_encoder_trainable: true
tabular_proj_trainable: true

# Image Encoder (Fantasma) - NECESARIO para evitar el error de inicialización
image_encoder_name: google/vit-base-patch16-224
image_encoder_pretrained: false
image_encoder_trainable: false
image_proj_trainable: false
image_embedding_dim: 768

# Multimodal Fusion
mm_encoder_trainable: true
mm_hidden_dim: 2048
mm_output_dim: 2048

# --- Paralelismo ---
# Configuración para una sola GPU
multiprocessing_distributed: false
ngpus_per_node: 1
world_size: 1
workers: 2